{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits\n",
    "A MAB problem can be interpreted as a Markov Decision Process (MDP) with a single state and multiple actions (i.e., arms). At each round, the learning agent executes an action (i.e., arm) and observes a random realization of the reward associated to the arm. The goal is to find the arm with the highest mean reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MAB problem is defined by $N$ arms. Each arm $i$ has an associated reward distribution $\\Lambda_i$ with mean $\\mu_i$.\n",
    "\n",
    "Let's define our first bandit problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your own bandit problem\n",
    "arm1 = arms.ArmBernoulli(0.30)\n",
    "arm2 = arms.ArmBernoulli(0.45)\n",
    "arm3 = arms.ArmBernoulli(0.15)\n",
    "arm4 = arms.ArmBernoulli(0.10)\n",
    "\n",
    "MAB = [arm1, arm2, arm3, arm4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_arms = len(MAB)\n",
    "means = [el.mean for el in MAB]\n",
    "\n",
    "# Display the means of your bandit (to find the best)\n",
    "print('means: {}'.format(means))\n",
    "mu_max = np.max(means)\n",
    "print('best arm: {}'.format(mu_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret\n",
    "We evaluate exploration-exploitation algorithms based on the expected regret.\n",
    "The **expected regret** measures the difference between executing the optimal arm $a^\\star$ and following the learning agent:\n",
    "$$R(T) = \\mathbb{E}\\Big[\\sum_{t=1}^T \\mu^\\star - r_t \\Big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore-then-commit\n",
    "ETC algorithms are composed by two phases. In the first phase, we do pure exploration in order to identify the problem (for example we execute a random policy). In the second phase, we act greedily based on the estimates.\n",
    "\n",
    "1. exploration phase\n",
    "At each time $t \\in [0, T_e]$, we select a random action $a_t \\sim \\mathcal{U}(N)$ and observe a reward $r_t \\sim \\Lambda_{a_t}$.\n",
    "\n",
    "We use this phase to build an estimate of the mean reward of each arm\n",
    "$$ \\hat{\\mu}_i = \\frac{1}{N_{T_e}(i)} \\sum_{t=0}^{T_e} r_t \\cdot 1(a_t = i)$$\n",
    "where $N_{T_e}(i)$ is the number of times arm $i$ was played.\n",
    "\n",
    "2. commit\n",
    "From $T_e$ on, we select $a_t = \\arg\\max_{i \\in [N]} \\hat{\\mu}_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = 200 # length of the pure exploration phase\n",
    "Tc = 200 # length of the commit phase\n",
    "T = Te + Tc\n",
    "\n",
    "regret = np.zeros((T,))\n",
    "\n",
    "cum_R = ...  # cumulative reward for each arm\n",
    "N = ...      # number of pulls for each arm\n",
    "for t in range(Te):\n",
    "    a = ...\n",
    "    reward = MAB[a].sample()\n",
    "    \n",
    "    # update statistics\n",
    "    N[a] += ...\n",
    "    cum_R[a] += ... \n",
    "    \n",
    "    # update regret (instantaneous regret)\n",
    "    regret[t] = ...\n",
    "    \n",
    "mu_hat = ...\n",
    "bestarm_hat = ...\n",
    "for t in range(Te, Te+Tc):\n",
    "    reward = MAB[bestarm_hat].sample()\n",
    "    \n",
    "    # update regret\n",
    "    regret[t] = ...\n",
    "\n",
    "# we want the cumulative regret\n",
    "cum_regret = np.cumsum(regret)\n",
    "\n",
    "plt.plot(cum_regret)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is wrong with that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm and environment (MAB problem) are stochastics. We want to have meaningful results, thus we average over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_repetitions = 40\n",
    "\n",
    "regret = np.zeros((nb_repetitions, T))\n",
    "\n",
    "for it in range(nb_repetitions):\n",
    "    ## here you should copy previous code\n",
    "    ## and change the upate of the regret\n",
    "\n",
    "cum_regret = np.cumsum(regret, axis=1)\n",
    "mean_regret = cum_regret.mean(axis=0)\n",
    "std = cum_regret.std(axis=0) / np.sqrt(nb_repetitions)\n",
    "\n",
    "plt.plot(mean_regret, label=\"ETC\")\n",
    "plt.fill_between(np.arange(T), mean_regret + std, mean_regret - std, alpha=0.1)\n",
    "plt.legend()\n",
    "\n",
    "# save current regret\n",
    "regret_ETC = mean_regret\n",
    "std_ETC= std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-Greedy\n",
    "This algorithm simply builds an estimate $\\hat{\\mu}_i$ of the mean reward of each arm and at each round it selects the action accorging to the following policy\n",
    "$$a_t = \\begin{cases}\n",
    "\\mathcal{U}(N) & w.p.~\\epsilon\\\\\n",
    "\\arg\\max_{i} \\hat{\\mu}_i & w.p.~1-\\epsilon\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = np.zeros((nb_repetitions, T))\n",
    "epsilon = 0.1\n",
    "\n",
    "for it in range(nb_repetitions):\n",
    "    ...\n",
    "    \n",
    "cum_regret = np.cumsum(regret, axis=1)\n",
    "mean_regret = cum_regret.mean(axis=0)\n",
    "std = cum_regret.std(axis=0) / np.sqrt(nb_repetitions)\n",
    "\n",
    "plt.plot(mean_regret, label=\"eps-greedy\")\n",
    "plt.fill_between(np.arange(T), mean_regret + std, mean_regret - std, alpha=0.1)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# save current regret\n",
    "regret_EPSGREEDY= mean_regret\n",
    "std_EPSGREEDY= std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The UCB1 algorithm\n",
    "\n",
    "The UCB1 algorithm is proposed by [Auer et al](https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf) for bandit instances with bounded rewards (in $[0,1]$ to fix the ideas). One can extend it to depend on some parameter $\\alpha$: \n",
    "\n",
    "$$A_{t} = \\underset{a}{\\text{argmax}} \\left[\\hat{\\mu}_a(t-1) + \\sqrt{\\frac{\\alpha \\log(t)}{N_a(t-1)}}\\right],$$\n",
    "where $\\hat{\\mu}_a(t)$ is the empirical mean of arm $a$ after $t$ rounds and $N_a(t)$ is the number of selections of arm $a$ till that time. \n",
    "\n",
    "UCB1 was originally proposed with $\\alpha = 2$. Its analysis was later refined to allow for $\\alpha > 1/2$ (see [here](http://sbubeck.com/Bubeckthesis.pdf) or [here](https://hal.archives-ouvertes.fr/hal-00738209/file/klucb.pdf)).\n",
    "\n",
    "* Implement UCB($\\alpha$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regret = np.zeros((nb_repetitions, T))\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "for it in range(nb_repetitions):\n",
    "    ...\n",
    "    \n",
    "cum_regret = np.cumsum(regret, axis=1)\n",
    "mean_regret = cum_regret.mean(axis=0)\n",
    "std = cum_regret.std(axis=0) / np.sqrt(nb_repetitions)\n",
    "\n",
    "plt.plot(mean_regret, label=\"UCB\")\n",
    "plt.fill_between(np.arange(T), mean_regret + std, mean_regret - std, alpha=0.1)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# save current regret\n",
    "regret_UCB = mean_regret\n",
    "std_UCB= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(regret_ETC, label=\"ETC\")\n",
    "plt.fill_between(np.arange(T), regret_UCB + std_UCB, regret_UCB - std_UCB, alpha=0.1)\n",
    "plt.plot(regret_EPSGREEDY, label=\"EPS GREEDY\")\n",
    "plt.fill_between(np.arange(T), regret_UCB + std_UCB, regret_UCB - std_UCB, alpha=0.1)\n",
    "plt.plot(regret_UCB, label=\"UCB\")\n",
    "plt.fill_between(np.arange(T), regret_UCB + std_UCB, regret_UCB - std_UCB, alpha=0.1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
